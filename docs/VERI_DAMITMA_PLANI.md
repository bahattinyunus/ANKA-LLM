# ğŸ§ª Veri DamÄ±tma ve Sentetik Veri Rafinerisi

Bu dokÃ¼man, milyar dolarlÄ±k donanÄ±m parkurlarÄ±na sahip olmadan, dÃ¼nya standartlarÄ±nda bir TÃ¼rkÃ§e LLM eÄŸitmenin en gerÃ§ekÃ§i yolu olan **"Knowledge Distillation" (Bilgi DamÄ±tma)** tekniklerini iÃ§erir.

## 1. Neden DamÄ±tma (Distillation)?

EÄŸer elinizde 20.000 adet H100 yoksa, modeli sÄ±fÄ±rdan eÄŸitmek (Pre-training) yerine, mevcut dev modellerin **mantÄ±k silsilesini (reasoning)** kÃ¼Ã§Ã¼k bir modele Ã¶ÄŸretmelisiniz.

* **Maliyet:** SÄ±fÄ±rdan eÄŸitim $100M+ iken, damÄ±tma yÃ¶ntemiyle ince ayar (Fine-tuning) $10k - $50k arasÄ±dÄ±r.
* **Verimlilik:** Dev modellerin iÃ§indeki "gÃ¼rÃ¼ltÃ¼" (noise) atÄ±lÄ±r, sadece "Ã¶z zeka" (signal) alÄ±nÄ±r.

---

## 2. Uygulama Stratejisi: "Teacher-Student" Modeli

### A. Ã–ÄŸretmen (Teacher) SeÃ§imi

En yÃ¼ksek muhakeme yeteneÄŸine sahip modeller "Ã–ÄŸretmen" olarak kullanÄ±lÄ±r.
* *SeÃ§enekler:* GPT-4o, Claude 3.5 Sonnet, Llama-3-70B.

### B. Ã–ÄŸrenci (Student) SeÃ§imi

TÃ¼rkiyeâ€™deki mevcut GPU altyapÄ±sÄ±nda (Ã–rn: 8x A100/H100) rahatÃ§a eÄŸitilebilecek ve son kullanÄ±cÄ±da (laptop/telefon) Ã§alÄ±ÅŸabilecek modeller.
* *SeÃ§enekler:* Llama-3-8B, Mistral-7B, Phi-3-Mini.

---

## 3. AdÄ±m AdÄ±m Veri DamÄ±tma Boru HattÄ± (Pipeline)

### 1. AdÄ±m: Soru Havuzu OluÅŸturma (Seed Tasks)

TÃ¼rkiye'ye Ã¶zgÃ¼ 10.000 temel gÃ¶rev belirlenir.
* *Ã–rnek:* "TÃ¼rk BorÃ§lar Kanunu'na gÃ¶re temerrÃ¼t ÅŸartlarÄ±nÄ± aÃ§Ä±kla."
* *Ã–rnek:* "Anadolu SelÃ§uklu Devleti'nin yÄ±kÄ±lÄ±ÅŸ sÃ¼recini sosyo-ekonomik aÃ§Ä±dan analiz et."

### 2. AdÄ±m: CoT (Chain of Thought) Ãœretimi

Ã–ÄŸretmen modele bu sorular sorulur ama sadece cevap istenmez. **DÃ¼ÅŸÃ¼nme aÅŸamalarÄ±nÄ±** (step-by-step reasoning) aÃ§Ä±klamasÄ± istenir.

### 3. AdÄ±m: Kalite Kontrol (Refining)

Ãœretilen cevaplar, kÃ¼Ã§Ã¼k bir yerli uzman grubu veya daha Ã¼st bir model tarafÄ±ndan "KÃ¼ltÃ¼rel Uygunluk" ve "DoÄŸruluk" testinden geÃ§irilir. YanlÄ±ÅŸ bilgi (hallucination) temizlenir.

### 4. AdÄ±m: SFT (Supervised Fine-Tuning)

Elde edilen bu "YÃ¼ksek Kaliteli TÃ¼rkÃ§e MantÄ±k Seti", Ã–ÄŸrenci modelimize (ANKA-7B) Ã¶ÄŸretilir. Model artÄ±k bir AmerikalÄ± gibi deÄŸil, bir TÃ¼rk uzman gibi dÃ¼ÅŸÃ¼nmeye baÅŸlar.

---

## ğŸ› ï¸ KullanÄ±lacak Teknik AraÃ§ Seti (Stack)

* **EÄŸitim KÃ¼tÃ¼phanesi:** [Unsloth](https://github.com/unslothai/unsloth) (Bellek kullanÄ±mÄ±nÄ± %80 azaltÄ±r, eÄŸitimi 2 kat hÄ±zlandÄ±rÄ±r).
* **Veri Ãœretim AraÃ§larÄ±:** [Distilabel](https://github.com/argilla-io/distilabel) (KarmaÅŸÄ±k damÄ±tma iÅŸ akÄ±ÅŸlarÄ±nÄ± otomatikleÅŸtirir).

---

## ğŸ“ˆ Beklenen SonuÃ§: "Mistral-TR" Moment

Bu aÅŸamanÄ±n sonunda elimizde;
1. Google Gemini kadar bÃ¼yÃ¼k olmayan ama,
2. **TÃ¼rkiye Ã¶zelindeki sorularda** Google'dan daha doÄŸru, daha hÄ±zlÄ± ve daha ucuz cevap veren bir model kalacaktÄ±r.

---

## ğŸš© Kritik UyarÄ±: KVKK ve Veri GizliliÄŸi

DamÄ±tma iÅŸlemi sÄ±rasÄ±nda kamuya aÃ§Ä±k olmayan, gizlilik dereceli veriler asla ticari API'lere (OpenAI vb.) gÃ¶nderilmemelidir. Bu tÃ¼r veriler iÃ§in iÃ§eride koÅŸturulan **Llama-3-70B** gibi aÃ§Ä±k kaynaklÄ± "Yerel Ã–ÄŸretmenler" kullanÄ±lmalÄ±dÄ±r.
