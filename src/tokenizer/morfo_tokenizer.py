from typing import List

class MorfoTokenizer:
    """
    Anka Silicon Dynamics TÃ¼rkÃ§e Morfolojik Tokenizer v2 (Advanced)
    
    TÃ¼rkÃ§enin sondan eklemeli yapÄ±sÄ±nÄ± analiz ederek, kelimeleri 
    kÃ¶k ve eklerine ayÄ±rÄ±r. Bu yÃ¶ntem;
    1. Kelime daÄŸarcÄ±ÄŸÄ±nÄ± (Vocab Size) %30 kÃ¼Ã§Ã¼ltÃ¼r.
    2. Modelin nadir kelimeleri (OOV) anlama kapasitesini artÄ±rÄ±r.
    """
    def __init__(self, vocab_size: int = 32000):
        self.vocab_size = vocab_size
        # GeliÅŸtirilmiÅŸ ek listesi (En uzundan en kÄ±saya doÄŸru sÄ±ralÄ± olmalÄ±)
        self.suffixes = sorted([
            # Ã‡oÄŸul ve Durum ekleri
            "lar", "ler", "dan", "den", "tan", "ten", "da", "de", "ta", "te", 
            "nÄ±n", "nin", "nun", "nÃ¼n", "Ä±n", "in", "un", "Ã¼n", "yÄ±", "yi", "yu", "Ã¼",
            # Ä°yelik ekleri
            "Ä±mÄ±z", "imiz", "leri", "larÄ±",
            # Zaman ekleri
            "iyor", "uyor", "Ä±yor", "Ã¼yÃ¶r", "acak", "ecek", "mÄ±ÅŸ", "miÅŸ", "muÅŸ", "mÃ¼ÅŸ",
            "dÄ±", "di", "du", "dÃ¼", "tÄ±", "ti", "tu", "tÃ¼",
            # YapÄ±m ekleri
            "lÄ±k", "lik", "luk", "lÃ¼k", "cÄ±", "ci", "cu", "cÃ¼", "lÄ±", "li", "lu", "lÃ¼", "sÄ±z", "siz"
        ], key=len, reverse=True)
        
        print(f"ğŸ‡¹ğŸ‡· ANKA Morfo-Tokenizer v2 HazÄ±r. | Hedef Vocab: {vocab_size}")

    def tokenize(self, text: str) -> List[str]:
        """
        Metni morfolojik birimlerine ayÄ±rÄ±r.
        Algoritma: Greedy Suffix Matching (AÃ§gÃ¶zlÃ¼ Ek EÅŸleÅŸtirme)
        """
        words = text.split()
        tokens = []
        for word in words:
            # Basit normalizasyon
            temp_word = word.lower().replace(".", "").replace(",", "")
            word_tokens = []
            
            current_stem = temp_word
            
            # Kelimeyi sondan baÅŸlayarak eklerine ayÄ±r (Recursive benzeri dÃ¶ngÃ¼)
            while len(current_stem) > 3: # KÃ¶k en az 3 harf olsun korumasÄ±
                found_suffix = False
                for suffix in self.suffixes:
                    if current_stem.endswith(suffix):
                        # Ek bulundu, ayÄ±r ve baÅŸa ekle
                        word_tokens.insert(0, "##" + suffix) # BPE/WordPiece tarzÄ± iÅŸaretleme
                        current_stem = current_stem[:-len(suffix)]
                        found_suffix = True
                        break # En uzun eki bulduÄŸumuz iÃ§in dÃ¶ngÃ¼yÃ¼ kÄ±r, yeni kÃ¶ke bak
                
                if not found_suffix:
                    break # Ek bulunamadÄ±ysa dÃ¶ngÃ¼yÃ¼ bitir
            
            word_tokens.insert(0, current_stem)
            tokens.extend(word_tokens)
            
        return tokens

    def decode(self, tokens: List[str]) -> str:
        """
        TokenlarÄ± birleÅŸtirir. '##' iÅŸaretini kaldÄ±rÄ±p birleÅŸtirir.
        """
        text = ""
        for token in tokens:
            if token.startswith("##"):
                text += token[2:]
            else:
                text += " " + token
        return text.strip()

if __name__ == "__main__":
    tokenizer = MorfoTokenizer()
    
    test_sentences = [
        "TÃ¼rkiye'nin geleceÄŸi kod satÄ±rlarÄ±nda gizlidir.",
        "BilgisayarcÄ±larÄ±mÄ±z algoritmalarÄ± geliÅŸtiriyorlar.",
        "Evsizlik ve iÅŸsizlik sorunlarÄ± Ã§Ã¶zÃ¼lecek."
    ]
    
    print("\n--- Morfo-Analiz Testi ---")
    for sent in test_sentences:
        tks = tokenizer.tokenize(sent)
        print(f"Girdi: {sent}")
        print(f"Token: {tks}\n")
